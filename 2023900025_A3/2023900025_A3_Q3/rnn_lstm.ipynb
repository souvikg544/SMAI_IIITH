{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38cbb5c-675b-47eb-b7f2-7321a86cc15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/souvikg544/miniconda3/envs/lrw/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0bf2e2-9f8d-4b1e-b86a-470f1a30f751",
   "metadata": {},
   "source": [
    "**Download necessary NLTK Packages**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3fc3fe-6972-4071-bfaf-e309100ba0dc",
   "metadata": {},
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8d2b8e-9c86-4df4-8d99-2bcd4ab26b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a29e548-f061-4821-a60a-eded85bd7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec4b1ab-b926-413e-9623-6771314e7aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197270dd-42b5-4c4a-9781-024e22530918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c69b3-b360-4c59-9474-ef88da6b0dd7",
   "metadata": {},
   "source": [
    "## Datasets and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a9e6ffd-d94d-41d6-8426-40e4336ad28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "def clean_n_remove_stop(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b19928e9-9d7e-4c8a-b7e3-2549596a5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text'].apply(clean_n_remove_stop)\n",
    "df = df[df['tokens'].map(len).between(100, 500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb39da85-39fc-4427-9cc3-81bfe07a277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(lambda x: [stemmer.stem(token) for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47930570-3400-4099-a337-615b99e3faa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10912 entries, 0 to 24998\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    10912 non-null  object\n",
      " 1   label   10912 non-null  int64 \n",
      " 2   tokens  10912 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 341.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c40a34f5-878c-4c71-8b1c-a558256353d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10912, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63415131-8b0e-48b5-97ce-7edb59720929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b40bafc1-a453-4478-a01f-be88627c0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter([word for tokens in df['tokens'] for word in tokens])\n",
    "vocab = {word: i+1 for i, (word, _) in enumerate(word_counts.most_common())}\n",
    "df['indexed_tokens'] = df['tokens'].apply(lambda x: [vocab[token] for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2a249c91-a087-4e44-a3e2-7eb5dd21bcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70598"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a0573b-5b90-4df0-bf67-2dc70e9c031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12523534-8be1-4912-a8ab-9965c8b80d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_df['tokens'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e76abbb-f089-4e7b-bb1c-ec6534c92b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, reviews, sentiments):\n",
    "        self.reviews = reviews\n",
    "        self.sentiments = sentiments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.reviews[idx]), torch.tensor(self.sentiments[idx])\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    xx_pad = torch.nn.utils.rnn.pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    return xx_pad, torch.tensor(yy), x_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad7bf834-6abe-4b70-8a68-235321f6b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(train_df['indexed_tokens'].to_numpy(), train_df['label'].to_numpy())\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_collate)\n",
    "val_dataset = IMDBDataset(val_df['indexed_tokens'].to_numpy(), val_df['label'].to_numpy())\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True, collate_fn=pad_collate)\n",
    "test_dataset = IMDBDataset(test_df['indexed_tokens'].to_numpy(), test_df['label'].to_numpy())\n",
    "test_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7617e0d-24f4-4c3f-b129-c41e4fc300d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7856"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1b665ab-ff69-456d-b33d-24afa9948ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bea8a242-2d5f-4492-bb8f-0d4c14c77f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82bd76cc-c987-4f49-bc44-e8caaafffb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  669,     2,  5063,  3199,   340,   996,  2145,    87,  3274,  5063,\n",
       "         5174, 63736, 25051, 15380, 14280,  6554,  1262,  6241,  1942,   443,\n",
       "           21,  3214, 25052, 30528,  2086,  7358,  1166,    86,    42,  2386,\n",
       "        63737,    23,  2050,   116,   714,  1166,  5063,    15,  2348,     1,\n",
       "          755,  9435,   626, 32011,  2050, 63738, 12624,   698,    27,   482,\n",
       "          356,    87,  3274,  5063,   187,     3,   397,    53,  4833,  1156,\n",
       "          160,   118,    89,   784,   957,   392,    87,  3274,   974,   274,\n",
       "          103,  2342,  3185, 63739,  6144,   694,  1363,  2514,  1225,   302,\n",
       "         8271,     1,    13,     2,  5063,  1021, 63740,  1572,   944,  1956,\n",
       "          499,  2742, 63741,    87,  1082,  1094,   580,  2454,  1257,  5285,\n",
       "         1207,   905,   838, 32342,  2266,    42,  5063,    43,  2651,   241,\n",
       "        11803, 32343, 12928, 63742, 13175,   765, 11855,  8551, 63743,  5174,\n",
       "        32343,   698,   360,   614,  1166, 12442,   522,     1,     1,    11,\n",
       "          756,     3,  5366, 12624,  3376,    43,  2651,  1429,    63,    55,\n",
       "           17,  5063,    23,   922,   922,  5366, 12624,   151, 63744, 20602,\n",
       "        32344, 63745,    96,   201,   118,    20,  1551,   187,  3773,     3,\n",
       "         2651,  5063, 32345, 63746, 17874,  2325, 12624,   187,    86, 32344,\n",
       "          452,  2150,  2266,  1166,  3001,    14,   132,  2050,   890,  2893,\n",
       "           22,    42,    14,  3968,     3,    27,  5366, 12624,   677,  5143,\n",
       "        32345,   323,     2,    61,  5432,  2454, 63747,     1,  1056,     3,\n",
       "         1134,   137,  3079,   111,    13,   411,     4,   152,     4,    86,\n",
       "         5063,     3, 22009,    27,  5366, 12624,  1094,  2454,   257,   252,\n",
       "        12461,  2651,  1930,   781,  3494,  1096, 63748,     1,  5063,    59,\n",
       "           19,   286,    16,    92,   125,    13,   131,    15,  3332,   214,\n",
       "            3,  9343,   302,    86,  2700,   957,   772,  2710,  6176,  1707,\n",
       "        63749,   292, 12880])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa9a58c-c5e4-4ae4-9243-d201f85e3211",
   "metadata": {},
   "source": [
    "##  RNN and LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d52ae184-36f5-4d0b-ac21-dd2b2af7307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0,output_strategy='last'):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.output_strategy = output_strategy\n",
    "        # if dropout:\n",
    "        #     self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        if self.output_strategy == 'last':\n",
    "            #print(hidden.squeeze(0).shape)\n",
    "            out = self.fc(hidden.squeeze(0))[-1]\n",
    "        elif self.output_strategy == 'mean':\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "            out = self.fc(output.mean(dim=1))\n",
    "        return out\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1, dropout=0.2, output_strategy='last'):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.output_strategy = output_strategy\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, _) = self.lstm(packed_embedded)\n",
    "        if self.output_strategy == 'last':\n",
    "            out = self.fc(hidden.squeeze(0))[-1]\n",
    "        elif self.output_strategy == 'mean':\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "            out = self.fc(output.mean(dim=1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d367438-bc17-46de-a1a9-75ec55fa1806",
   "metadata": {},
   "source": [
    "**Training with Binary Cross Entropy Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb825d87-d93d-4316-8125-7255e5921522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bce(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels, lengths in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device, dtype=torch.float)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, lengths).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * train_correct / total\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        val_loss, val_accuracy = evaluate_bce(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        wandb.log({\"epoch\": epoch+1, \"train_loss\": train_loss, \"train_accuracy\": train_accuracy,\n",
    "                   \"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n",
    "\n",
    "def evaluate_bce(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, lengths in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs, lengths).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * total_correct / total\n",
    "    total_loss /= len(dataloader)\n",
    "    return total_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d25abb0e-fc32-4602-bc5c-71da7895a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNNModel(len(vocab) + 1, config.embedding_dim, config.hidden_dim, config.output_dim, config.num_layers, config.dropout)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# # Train the model\n",
    "# train(model, train_loader, val_loader, criterion, optimizer, num_epochs=config.num_epochs)\n",
    "\n",
    "# # Evaluate on test set\n",
    "# test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
    "# print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "# wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03408f19-a557-46f3-8085-3d3feb41fe56",
   "metadata": {},
   "source": [
    "**Training with Cross Entropy**\n",
    "With experimentations we note that both the losses perform equally, hence we stick with Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "13217f34-27c1-47ff-9d6a-bc5259dc24ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ce(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels, lengths in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, lengths).softmax(dim=1)\n",
    "            #print(outputs.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            #predicted = torch.round(torch.sigmoid(outputs))\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * train_correct / total\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        val_loss, val_accuracy = evaluate_ce(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        wandb.log({\"epoch\": epoch+1, \"train_loss\": train_loss, \"train_accuracy\": train_accuracy,\n",
    "                   \"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n",
    "\n",
    "def evaluate_ce(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, lengths in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * total_correct / total\n",
    "    total_loss /= len(dataloader)\n",
    "    return total_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe9e86-ffda-4860-8ce4-262d16a4e4da",
   "metadata": {},
   "source": [
    "**In the following experiments we note that taking the mean of all the intermediate outputs greatly improves the performance of both LSTM and RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904317d0-f116-4b0b-ac5e-568b4b0ae382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"Imdb_rnn_classification\")\n",
    "# config = wandb.config\n",
    "# config.embedding_dim = 100\n",
    "# config.hidden_dim = 200\n",
    "# config.output_dim = 2\n",
    "# config.num_layers = 2\n",
    "# config.dropout = 0.1\n",
    "# config.learning_rate = 0.001\n",
    "# config.num_epochs = 20\n",
    "# config.batch_size = 64\n",
    "# config.output_strategy = 'mean'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb96ac-edd8-4648-8d48-3b3ffaae0bff",
   "metadata": {},
   "source": [
    "After different hyperparameter tuning we find the below setup to be apt for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "91a7c2ae-9bc2-4f1f-9dd5-5ad6e2d387c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:h27wcfmg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▁▂▂▁▂▂▁▂▂▁▁▂▁▁▂▂▁▂▂▃▃▄▄▄▅▅▆▆▇▇██▁▂▂▁</td></tr><tr><td>test_accuracy</td><td>█▃▃▂▁▆▇</td></tr><tr><td>test_loss</td><td>▁▁█▅▂▁▁</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅██▁</td></tr><tr><td>train_loss</td><td>████████████████████████████████████▅▁▁█</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇██▁</td></tr><tr><td>val_loss</td><td>▃▃▃▃▃▃▃▄▄▆▃▃▅▃▃▃▃▃▃▃▄▃▃▄▄▄▅▄▄▄▄▄▅▄▃▄▁▄█▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>8</td></tr><tr><td>test_accuracy</td><td>51.0947</td></tr><tr><td>test_loss</td><td>0.69143</td></tr><tr><td>train_accuracy</td><td>49.46538</td></tr><tr><td>train_loss</td><td>0.69327</td></tr><tr><td>val_accuracy</td><td>50.63001</td></tr><tr><td>val_loss</td><td>0.69282</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-meadow-9</strong> at: <a href='https://wandb.ai/rockon/imdb_rnn_classification/runs/h27wcfmg' target=\"_blank\">https://wandb.ai/rockon/imdb_rnn_classification/runs/h27wcfmg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240328_150046-h27wcfmg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:h27wcfmg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/souvikg544/souvik/2023900025_A3/2023900025_A3_Q3/wandb/run-20240328_170945-bkba3qyb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rockon/imdb_rnn_classification/runs/bkba3qyb' target=\"_blank\">sparkling-totem-10</a></strong> to <a href='https://wandb.ai/rockon/imdb_rnn_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rockon/imdb_rnn_classification' target=\"_blank\">https://wandb.ai/rockon/imdb_rnn_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rockon/imdb_rnn_classification/runs/bkba3qyb' target=\"_blank\">https://wandb.ai/rockon/imdb_rnn_classification/runs/bkba3qyb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Imdb_rnn_classification\")\n",
    "config = wandb.config\n",
    "config.embedding_dim = 128\n",
    "config.hidden_dim = 256\n",
    "config.output_dim = 2\n",
    "config.num_layers = 3\n",
    "config.dropout = 0.2\n",
    "config.learning_rate = 0.001\n",
    "config.num_epochs = 20\n",
    "config.batch_size = 64\n",
    "config.output_strategy = 'mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "94aee785-08b4-40b5-b274-317de9118147",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn_last = RNNModel(len(vocab) + 1, config.embedding_dim, config.hidden_dim, config.output_dim, config.num_layers, config.dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_rnn_last = optim.Adam(model_rnn_last.parameters(), lr=config.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "18cd4008-a11e-42a5-8f17-2d86b0bbfdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.6864, Train Accuracy: 56.58%, Val Loss: 0.6809, Val Accuracy: 57.16%\n",
      "Epoch 2/20, Train Loss: 0.6635, Train Accuracy: 61.25%, Val Loss: 0.6895, Val Accuracy: 55.78%\n",
      "Epoch 3/20, Train Loss: 0.6880, Train Accuracy: 56.92%, Val Loss: 1.4029, Val Accuracy: 53.49%\n",
      "Epoch 4/20, Train Loss: 0.6543, Train Accuracy: 62.89%, Val Loss: 0.7608, Val Accuracy: 60.82%\n",
      "Epoch 5/20, Train Loss: 0.5987, Train Accuracy: 70.02%, Val Loss: 1.0922, Val Accuracy: 67.01%\n",
      "Epoch 6/20, Train Loss: 0.6365, Train Accuracy: 63.81%, Val Loss: 1.0668, Val Accuracy: 57.73%\n",
      "Epoch 7/20, Train Loss: 0.6961, Train Accuracy: 52.46%, Val Loss: 0.7744, Val Accuracy: 52.35%\n",
      "Epoch 8/20, Train Loss: 0.6942, Train Accuracy: 52.30%, Val Loss: 0.7540, Val Accuracy: 52.92%\n",
      "Epoch 9/20, Train Loss: 0.6878, Train Accuracy: 53.88%, Val Loss: 0.7917, Val Accuracy: 47.31%\n",
      "Epoch 10/20, Train Loss: 0.6891, Train Accuracy: 53.17%, Val Loss: 0.7398, Val Accuracy: 56.59%\n",
      "Epoch 11/20, Train Loss: 0.6639, Train Accuracy: 59.00%, Val Loss: 0.8192, Val Accuracy: 56.24%\n",
      "Epoch 12/20, Train Loss: 0.6325, Train Accuracy: 64.26%, Val Loss: 0.8358, Val Accuracy: 60.25%\n",
      "Epoch 13/20, Train Loss: 0.5927, Train Accuracy: 70.20%, Val Loss: 1.1937, Val Accuracy: 62.54%\n",
      "Epoch 14/20, Train Loss: 0.5722, Train Accuracy: 72.11%, Val Loss: 1.0330, Val Accuracy: 64.38%\n",
      "Epoch 15/20, Train Loss: 0.5333, Train Accuracy: 77.07%, Val Loss: 1.3187, Val Accuracy: 66.32%\n",
      "Epoch 16/20, Train Loss: 0.5456, Train Accuracy: 75.79%, Val Loss: 1.4505, Val Accuracy: 66.21%\n",
      "Epoch 17/20, Train Loss: 0.5272, Train Accuracy: 77.93%, Val Loss: 1.5563, Val Accuracy: 65.98%\n",
      "Epoch 18/20, Train Loss: 0.5013, Train Accuracy: 80.70%, Val Loss: 1.4519, Val Accuracy: 66.09%\n",
      "Epoch 19/20, Train Loss: 0.5109, Train Accuracy: 79.81%, Val Loss: 1.9539, Val Accuracy: 64.72%\n",
      "Epoch 20/20, Train Loss: 0.4879, Train Accuracy: 82.09%, Val Loss: 1.7902, Val Accuracy: 63.23%\n",
      "Test Loss: 0.2164, Test Accuracy: 98.08%\n"
     ]
    }
   ],
   "source": [
    "train_ce(model_rnn_last, train_loader, val_loader, criterion, optimizer_rnn_last, num_epochs=config.num_epochs)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = evaluate_ce(model_rnn, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "863c2c7d-0111-4173-82d1-061c818db91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn_mean = RNNModel(len(vocab) + 1, config.embedding_dim, config.hidden_dim, config.output_dim, config.num_layers, config.dropout,config.output_strategy)\n",
    "optimizer_rnn_mean = optim.Adam(model_rnn_mean.parameters(), lr=config.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2978d60a-564a-49dd-828d-888bc6cc97d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.6206, Train Accuracy: 66.26%, Val Loss: 0.5839, Val Accuracy: 72.28%\n",
      "Epoch 2/20, Train Loss: 0.5272, Train Accuracy: 77.48%, Val Loss: 0.7714, Val Accuracy: 75.49%\n",
      "Epoch 3/20, Train Loss: 0.4680, Train Accuracy: 83.99%, Val Loss: 0.5309, Val Accuracy: 79.61%\n",
      "Epoch 4/20, Train Loss: 0.4289, Train Accuracy: 88.23%, Val Loss: 0.7682, Val Accuracy: 79.84%\n",
      "Epoch 5/20, Train Loss: 0.3985, Train Accuracy: 91.48%, Val Loss: 0.9484, Val Accuracy: 80.07%\n",
      "Epoch 6/20, Train Loss: 0.3783, Train Accuracy: 93.65%, Val Loss: 0.9752, Val Accuracy: 80.99%\n",
      "Epoch 7/20, Train Loss: 0.3688, Train Accuracy: 94.59%, Val Loss: 1.2428, Val Accuracy: 80.76%\n",
      "Epoch 8/20, Train Loss: 0.3615, Train Accuracy: 95.35%, Val Loss: 1.4530, Val Accuracy: 80.87%\n",
      "Epoch 9/20, Train Loss: 0.3574, Train Accuracy: 95.75%, Val Loss: 1.5294, Val Accuracy: 80.64%\n",
      "Epoch 10/20, Train Loss: 0.3551, Train Accuracy: 95.93%, Val Loss: 1.1137, Val Accuracy: 80.76%\n",
      "Epoch 11/20, Train Loss: 0.3514, Train Accuracy: 96.26%, Val Loss: 1.9283, Val Accuracy: 80.30%\n",
      "Epoch 12/20, Train Loss: 0.4999, Train Accuracy: 80.16%, Val Loss: 0.5771, Val Accuracy: 70.90%\n",
      "Epoch 13/20, Train Loss: 0.5284, Train Accuracy: 77.28%, Val Loss: 0.7202, Val Accuracy: 78.12%\n",
      "Epoch 14/20, Train Loss: 0.5143, Train Accuracy: 79.51%, Val Loss: 0.6756, Val Accuracy: 80.07%\n",
      "Epoch 15/20, Train Loss: 0.4108, Train Accuracy: 89.82%, Val Loss: 0.4589, Val Accuracy: 78.01%\n",
      "Epoch 16/20, Train Loss: 0.3842, Train Accuracy: 93.47%, Val Loss: 0.9067, Val Accuracy: 80.99%\n",
      "Epoch 17/20, Train Loss: 0.3654, Train Accuracy: 94.86%, Val Loss: 0.9167, Val Accuracy: 82.02%\n",
      "Epoch 18/20, Train Loss: 0.3571, Train Accuracy: 95.61%, Val Loss: 1.3553, Val Accuracy: 80.53%\n",
      "Epoch 19/20, Train Loss: 0.3499, Train Accuracy: 96.38%, Val Loss: 0.9340, Val Accuracy: 82.47%\n",
      "Epoch 20/20, Train Loss: 0.3432, Train Accuracy: 97.11%, Val Loss: 1.0596, Val Accuracy: 83.05%\n",
      "Test Loss: 0.2164, Test Accuracy: 98.08%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_ce(model_rnn_mean, train_loader, val_loader, criterion, optimizer_rnn_mean, num_epochs=config.num_epochs)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = evaluate_ce(model_rnn_mean, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa000c-e27d-4ce4-8cea-b0bda38e7744",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "012c2f02-6650-43ec-85a6-5757bd15cc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bkba3qyb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▄▄▅▅▆▇▇██▁▂▂▃▄▄▅▅▆▆▇▇█▁▂▂▃▃▄▄▅▅▆▇▇█</td></tr><tr><td>test_accuracy</td><td>▁▁▁</td></tr><tr><td>test_loss</td><td>█▁▁</td></tr><tr><td>train_accuracy</td><td>▂▁▂▁▄▆▇▇██████▂▂▄▃▁▁▂▃▄▅▅▅▆▃▆▇▇▇██▅▅▇▇██</td></tr><tr><td>train_loss</td><td>████▆▄▃▂▁▁▁▁▁▁▇█▆▇██▇▇▆▅▅▄▄▇▄▃▂▂▁▁▄▅▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▃▃▂▂▆▇████████▃▂▅▃▂▁▃▄▄▅▅▅▄▆▇▇████▆▇▇█▇█</td></tr><tr><td>val_loss</td><td>▅▂▃▂▂▁▂▃▃▃▃▃▄▄▂▆▄▄▃▃▃▃▄▆▇▆█▂▁▃▄▅▇▄▂▂▁▃▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>test_accuracy</td><td>98.0779</td></tr><tr><td>test_loss</td><td>0.21643</td></tr><tr><td>train_accuracy</td><td>97.11049</td></tr><tr><td>train_loss</td><td>0.34319</td></tr><tr><td>val_accuracy</td><td>83.04696</td></tr><tr><td>val_loss</td><td>1.05965</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sparkling-totem-10</strong> at: <a href='https://wandb.ai/rockon/imdb_rnn_classification/runs/bkba3qyb' target=\"_blank\">https://wandb.ai/rockon/imdb_rnn_classification/runs/bkba3qyb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240328_170945-bkba3qyb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bkba3qyb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/souvikg544/souvik/2023900025_A3/2023900025_A3_Q3/wandb/run-20240328_173659-7rfp07fl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rockon/Imdb_LSTM_classification/runs/7rfp07fl' target=\"_blank\">tough-bird-1</a></strong> to <a href='https://wandb.ai/rockon/Imdb_LSTM_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rockon/Imdb_LSTM_classification' target=\"_blank\">https://wandb.ai/rockon/Imdb_LSTM_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rockon/Imdb_LSTM_classification/runs/7rfp07fl' target=\"_blank\">https://wandb.ai/rockon/Imdb_LSTM_classification/runs/7rfp07fl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Imdb_LSTM_classification\")\n",
    "config = wandb.config\n",
    "config.embedding_dim = 128\n",
    "config.hidden_dim = 256\n",
    "config.output_dim = 2\n",
    "config.num_layers = 3\n",
    "config.dropout = 0.2\n",
    "config.learning_rate = 0.001\n",
    "config.num_epochs = 20\n",
    "config.batch_size = 64\n",
    "config.output_strategy = 'mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "359d0587-399c-4174-a6d2-10b47f324108",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = LSTMModel(len(vocab) + 1, config.embedding_dim, config.hidden_dim, config.output_dim, config.num_layers, config.dropout,config.output_strategy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=config.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "82194830-0028-4cc1-9196-12b73a1f63ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.6164, Train Accuracy: 66.41%, Val Loss: 0.6079, Val Accuracy: 74.11%\n",
      "Epoch 2/20, Train Loss: 0.5003, Train Accuracy: 80.60%, Val Loss: 0.6832, Val Accuracy: 78.69%\n",
      "Epoch 3/20, Train Loss: 0.4427, Train Accuracy: 86.77%, Val Loss: 0.6498, Val Accuracy: 82.36%\n",
      "Epoch 4/20, Train Loss: 0.4058, Train Accuracy: 90.85%, Val Loss: 0.8607, Val Accuracy: 82.13%\n",
      "Epoch 5/20, Train Loss: 0.3853, Train Accuracy: 92.82%, Val Loss: 0.8301, Val Accuracy: 82.13%\n",
      "Epoch 6/20, Train Loss: 0.3693, Train Accuracy: 94.53%, Val Loss: 1.0716, Val Accuracy: 81.33%\n",
      "Epoch 7/20, Train Loss: 0.3540, Train Accuracy: 96.07%, Val Loss: 1.1751, Val Accuracy: 80.64%\n",
      "Epoch 8/20, Train Loss: 0.3523, Train Accuracy: 96.32%, Val Loss: 1.1727, Val Accuracy: 82.59%\n",
      "Epoch 9/20, Train Loss: 0.3468, Train Accuracy: 96.74%, Val Loss: 1.0950, Val Accuracy: 82.25%\n",
      "Epoch 10/20, Train Loss: 0.3427, Train Accuracy: 97.19%, Val Loss: 1.3791, Val Accuracy: 80.30%\n",
      "Epoch 11/20, Train Loss: 0.3418, Train Accuracy: 97.23%, Val Loss: 1.6087, Val Accuracy: 80.64%\n",
      "Epoch 12/20, Train Loss: 0.3394, Train Accuracy: 97.44%, Val Loss: 1.6784, Val Accuracy: 84.19%\n",
      "Epoch 13/20, Train Loss: 0.3398, Train Accuracy: 97.38%, Val Loss: 1.2524, Val Accuracy: 84.19%\n",
      "Epoch 14/20, Train Loss: 0.3395, Train Accuracy: 97.45%, Val Loss: 1.2978, Val Accuracy: 83.39%\n",
      "Epoch 15/20, Train Loss: 0.3376, Train Accuracy: 97.65%, Val Loss: 1.3495, Val Accuracy: 83.96%\n",
      "Epoch 16/20, Train Loss: 0.3380, Train Accuracy: 97.54%, Val Loss: 1.4366, Val Accuracy: 82.70%\n",
      "Epoch 17/20, Train Loss: 0.3360, Train Accuracy: 97.79%, Val Loss: 1.3379, Val Accuracy: 84.42%\n",
      "Epoch 18/20, Train Loss: 0.3358, Train Accuracy: 97.77%, Val Loss: 1.3034, Val Accuracy: 84.42%\n",
      "Epoch 19/20, Train Loss: 0.3384, Train Accuracy: 97.51%, Val Loss: 1.4063, Val Accuracy: 83.62%\n",
      "Epoch 20/20, Train Loss: 0.3350, Train Accuracy: 97.86%, Val Loss: 1.3582, Val Accuracy: 83.39%\n",
      "Test Loss: 0.3806, Test Accuracy: 98.09%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_ce(model_lstm, train_loader, val_loader, criterion, optimizer_lstm, num_epochs=config.num_epochs)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = evaluate_ce(model_lstm, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b118c4-fd37-4a09-a484-7e579644cebb",
   "metadata": {},
   "source": [
    "**We find that LSTMs beat RNNs due to their ability to recall long contexts which is essential in use cases like sentiment analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6ead6-e4cd-46fa-96d3-b28027c6df21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrw",
   "language": "python",
   "name": "lrw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
